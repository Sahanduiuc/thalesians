{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import talk.config as con\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "con.config_mosek()\n",
    "con.config_matplotlib()\n",
    "con.config_configManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constrained regression\n",
    "\n",
    "Let $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ and $\\mathbf{b} \\in \\mathbb{R}^n$. We solve the constrained least squares problem: \n",
    "\n",
    "\\begin{align}\\mathbf{x}^{*}=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^m}& \\rVert{\\mathbf{A}\\mathbf{x}-\\mathbf{b}}\\lVert_2\\\\\n",
    "\\text{s.t. } &\\Sigma\\,x_i=1\\\\\n",
    "            &\\mathbf{x}\\geq 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples:\n",
    "- Tracking an index (index in $\\mathbf{b}$, assets in $\\mathbf{A}$)\n",
    "- Constructing an indicator, factor analyis, ...\n",
    "- Approximation...\n",
    "- ...\n",
    "\n",
    "Regression is the **Swiss army knife** of professional quant finance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thales of Miletus (c. 624 BC -  c. 546 BC). \n",
    "<img src=\"talk/thales.jpg\" style=\"margin-left:auto; margin-right:auto; display:block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The normal equations\n",
    "\n",
    "As we (probably) all know \n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{*}=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1}\\mathbf{A}^{T}\\mathbf{x}\n",
    "$$\n",
    "solves\n",
    "\\begin{align}\\mathbf{x}^{*}=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^m}& \\rVert{\\mathbf{A}\\mathbf{x}-\\mathbf{b}}\\lVert_2\n",
    "\\end{align}\n",
    "Almost there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shall we apply the sculptor method?\n",
    "- We could delete the negative entries (really bad if they are all negative)\n",
    "- We could scale the surviving entries to enforce the $\\Sigma\\,x_i=1$.\n",
    "\n",
    "Done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"talk/cone.png\" style=\"margin-left:auto; margin-right:auto; display:block\">\n",
    "$$y \\geq \\sqrt{x_1^2 + x_2^2}=\\rVert{\\mathbf{x}}\\lVert_2$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conic Programming\n",
    "\n",
    "We introduce an auxiliary scalar $z$:\n",
    "    \n",
    "\\begin{align}\\min_{z \\in \\mathbb{R}, \\mathbf{x} \\in \\mathbb{R}^m} & z\\\\\n",
    "\\text{s.t. }&z \\geq \\rVert{\\mathbf{A}\\mathbf{x}-\\mathbf{b}}\\lVert_2\\\\\n",
    "            &\\Sigma\\,x_i=1\\\\\n",
    "            &\\mathbf{x}\\geq 0\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We introduce an auxiliary vector $\\mathbf{y} \\in \\mathbb{R}^n$:\n",
    "\n",
    "\\begin{align}\\min_{z \\in \\mathbb{R}, \\mathbf{x} \\in \\mathbb{R}^m, \\mathbf{y} \\in \\mathbb{R}^n} & z\\\\\n",
    "\\text{s.t. }&z \\geq \\rVert{y}\\lVert_2\\\\\n",
    "            &\\mathbf{y} = \\mathbf{A}\\mathbf{x}-\\mathbf{b}\\\\\n",
    "            &\\Sigma\\,x_i=1\\\\\n",
    "            &\\mathbf{x}\\geq 0\n",
    "\\end{align}\n",
    "\n",
    "We **lifted** the problem from a $m$ dimensional space into a $m + n + 1$ dimensional space. \n",
    "\n",
    "Alternative notation: $$z \\geq \\rVert{y}\\lVert_2 \\,\\Leftrightarrow\\, [z,y] \\in \\mathcal{Q}_{n+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application: Implementing a minimum variance portfolio\n",
    "The $i$th column of $\\mathbf{R}$ is the time series of returns for the $i$th asset.\n",
    "Hence to minimize the variance of a portfolio (a linear combination of assets) we solve:\n",
    "\n",
    "\\begin{align}\\mathbf{w}^{*}=\\arg\\min_{\\mathbf{w} \\in \\mathbb{R}^m}& \\rVert{\\mathbf{R}\\mathbf{w}-\\mathbf{0}}\\lVert_2\\\\\n",
    "\\text{s.t. } &\\Sigma\\,w_i=1\\\\\n",
    "            &\\mathbf{w}\\geq 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mosek.fusion import Expr, Domain, Model, DenseMatrix, ObjectiveSense\n",
    "import numpy as np\n",
    "\n",
    "def min_var(matrix, lamb=0.0):\n",
    "    \"\"\"\n",
    "    min 2-norm (matrix*w) - lamb*2-norm(w)\n",
    "    s.t. e'w = 1, w >= 0\n",
    "    \"\"\"\n",
    "    # define model\n",
    "    model = Model('lsqPos')\n",
    "    \n",
    "    # introduce non-negative weight variables\n",
    "    w = model.variable(\"w\", int(matrix.shape[1]), Domain.inRange(0.0, 1.0))\n",
    "    \n",
    "    # introduce 1 upper bound for the cone\n",
    "    z = model.variable(\"cone\", 1, Domain.unbounded())\n",
    "    t = model.variable(\"tikhonov\", 1, Domain.unbounded())\n",
    "    \n",
    "    # e'*w = 1\n",
    "    model.constraint(Expr.sum(w), Domain.equalsTo(1.0))\n",
    "    \n",
    "    # introduce the cone (e.g. upper bound for the norm of y=A*w)\n",
    "    model.constraint(Expr.vstack(z, Expr.mul(DenseMatrix(matrix), w)), Domain.inQCone())\n",
    "    \n",
    "    model.constraint(Expr.vstack(t, w), Domain.inQCone())\n",
    "    # minimization of the residual\n",
    "    model.objective(ObjectiveSense.Minimize, Expr.add(z, Expr.mul(t, lamb)))\n",
    "    model.solve()\n",
    "\n",
    "    return np.array(w.level())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "random_data = np.dot(np.random.randn(250,5), np.diag([1,2,3,4,5]))\n",
    "data = min_var(random_data)\n",
    "print(data)\n",
    "\n",
    "plt.bar(range(0,5),data)\n",
    "plt.ylabel(\"Weight\"), plt.xlabel(\"index\"), plt.title(\"Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Balance?\n",
    "\n",
    "- Bounds\n",
    "- **Tikhonov regularization** (penalty by the $2$-norm of the weights in the objective), also known as **Ridge Regression** or **Shrinkage to the mean**\n",
    " \n",
    "\n",
    "\\begin{align}\\mathbf{w}^{*}=\\arg\\min_{\\mathbf{w} \\in \\mathbb{R}^m}& \\rVert{\\mathbf{R}\\mathbf{w}}\\lVert_2 + \\lambda \\rVert{\\mathbf{w}}\\lVert_2\\\\\n",
    "\\text{s.t. } &\\Sigma\\,w_i=1\\\\\n",
    "            &\\mathbf{w}\\geq 0\n",
    "\\end{align}\n",
    "\n",
    "- The $1/N$ portfolio is the limit for $\\lambda \\to \\infty$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for lamb in [0.0, 5.0, 10.0, 20.0, 50.0, 100.0, 200.0]:\n",
    "    data = min_var(random_data, lamb=lamb)\n",
    "    plt.bar(range(0,5), data)\n",
    "    plt.title(lamb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- We solve a constrained least squares problem by introducing $n + 1$ additional dimensions ($n$ is the number of rows is the problem).\n",
    "\n",
    "- We introduce a quadratric cone living in those new dimensions.\n",
    "\n",
    "- We construct a minimum variance portfolio.\n",
    "\n",
    "- Using Tikhonov regularization we can interpolate between the Minimum Variance portfolio and the $1/N$ portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=http://localhost:8888>Back to Overview</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
